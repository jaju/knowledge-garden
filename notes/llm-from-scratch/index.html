<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.7.33">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Notes: LLM From Scratch – M’Sync</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-html/quarto.js" type="module"></script>
<script src="../../site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-ea385d0e468b0dd5ea5bf0780b1290d9.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap-d64559d910ec87c4ee681e1e2f97436f.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>


<link rel="stylesheet" href="../../styles/notes-style.css">
<link rel="stylesheet" href="styles.css">
</head>

<body class="nav-fixed quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">M’Sync</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../notes/index.html"> 
<span class="menu-text">Notes</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../blog/index.html"> 
<span class="menu-text">Blog</span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#synopsis" id="toc-synopsis" class="nav-link active" data-scroll-target="#synopsis"><span class="header-section-number">1</span> Synopsis</a></li>
  <li><a href="#sec-chapter-1" id="toc-sec-chapter-1" class="nav-link" data-scroll-target="#sec-chapter-1"><span class="header-section-number">2</span> Chapter 1: Understanding Large Language Models</a>
  <ul class="collapse">
  <li><a href="#key-challenges-in-pre-training" id="toc-key-challenges-in-pre-training" class="nav-link" data-scroll-target="#key-challenges-in-pre-training"><span class="header-section-number">2.1</span> Key challenges in pre-training</a></li>
  </ul></li>
  <li><a href="#chapter-2-working-with-text-data" id="toc-chapter-2-working-with-text-data" class="nav-link" data-scroll-target="#chapter-2-working-with-text-data"><span class="header-section-number">3</span> Chapter 2: Working with text data</a>
  <ul class="collapse">
  <li><a href="#tokenization" id="toc-tokenization" class="nav-link" data-scroll-target="#tokenization"><span class="header-section-number">3.1</span> Tokenization</a>
  <ul class="collapse">
  <li><a href="#v1" id="toc-v1" class="nav-link" data-scroll-target="#v1"><span class="header-section-number">3.1.1</span> V1</a></li>
  <li><a href="#v2" id="toc-v2" class="nav-link" data-scroll-target="#v2"><span class="header-section-number">3.1.2</span> V2</a></li>
  <li><a href="#v3---byte-pair-encoding-bpe" id="toc-v3---byte-pair-encoding-bpe" class="nav-link" data-scroll-target="#v3---byte-pair-encoding-bpe"><span class="header-section-number">3.1.3</span> V3 - Byte Pair Encoding (BPE)</a></li>
  <li><a href="#v4" id="toc-v4" class="nav-link" data-scroll-target="#v4"><span class="header-section-number">3.1.4</span> V4</a></li>
  </ul></li>
  <li><a href="#embeddings" id="toc-embeddings" class="nav-link" data-scroll-target="#embeddings"><span class="header-section-number">3.2</span> Embeddings</a>
  <ul class="collapse">
  <li><a href="#what-do-they-look-like" id="toc-what-do-they-look-like" class="nav-link" data-scroll-target="#what-do-they-look-like"><span class="header-section-number">3.2.1</span> What do they look like?</a></li>
  <li><a href="#v0" id="toc-v0" class="nav-link" data-scroll-target="#v0"><span class="header-section-number">3.2.2</span> V0</a></li>
  <li><a href="#v1-1" id="toc-v1-1" class="nav-link" data-scroll-target="#v1-1"><span class="header-section-number">3.2.3</span> V1</a></li>
  <li><a href="#v2---using-word-positions" id="toc-v2---using-word-positions" class="nav-link" data-scroll-target="#v2---using-word-positions"><span class="header-section-number">3.2.4</span> V2 - Using word positions</a></li>
  </ul></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<div class="quarto-title-block"><div><h1 class="title">Notes: LLM From Scratch</h1><button type="button" class="btn code-tools-button" id="quarto-code-tools-source"><i class="bi"></i> Code</button></div></div>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<section id="synopsis" class="level1" data-number="1">
<h1 data-number="1"><span class="header-section-number">1</span> Synopsis</h1>
<p>Notes and pointers during the exploration of the book <a href="https://www.manning.com/books/build-a-large-language-model-from-scratch">LLM From Scratch</a> by <a href="https://github.com/rasbt">Sebastian Raschka</a>. While I am using this book as the central reference, the notes will contain additional references and pointers to other resources.</p>
</section>
<section id="sec-chapter-1" class="level1" data-number="2">
<h1 data-number="2"><span class="header-section-number">2</span> Chapter 1: Understanding Large Language Models</h1>
<ul>
<li>A good quality corpus is crucial.</li>
<li>Choice of corpus will depend on the downstream tasks.</li>
<li>For example, to generate code, the corpus should contain a substantial amount of code snippets.</li>
</ul>
<p>An open-source corpus of 3 trillion tokens has been detailed in <span class="citation" data-cites="dolma2024">(<a href="#ref-dolma2024" role="doc-biblioref">Soldaini et al. 2024</a>)</span>.</p>
<section id="key-challenges-in-pre-training" class="level2" data-number="2.1">
<h2 data-number="2.1" class="anchored" data-anchor-id="key-challenges-in-pre-training"><span class="header-section-number">2.1</span> Key challenges in pre-training</h2>
<ul>
<li>Data: Creating or curating a large corpus of text data is a significant challenge. The quality and diversity of the data are crucial for the performance of the model.</li>
<li>Model: The model architecture is key for pre-training. A good model architecture will help in learning the underlying structure of the data.</li>
<li>Budget: Pre-training a large language model requires a lot of computational resources. A good budget is crucial for pre-training.</li>
<li><a href="https://en.wikipedia.org/wiki/GPT-3">GPT-3</a> potentially cost $4.6 million to train, per <span class="citation" data-cites="gpt3technicaloverview">Li (<a href="#ref-gpt3technicaloverview" role="doc-biblioref">2020</a>)</span>.</li>
</ul>
<p><span class="citation" data-cites="instruct-gpt">Ouyang et al. (<a href="#ref-instruct-gpt" role="doc-biblioref">2022</a>)</span> presented ideas on how to fine-tune GPT-3 on a dataset of instructions.</p>
</section>
</section>
<section id="chapter-2-working-with-text-data" class="level1" data-number="3">
<h1 data-number="3"><span class="header-section-number">3</span> Chapter 2: Working with text data</h1>
<p>For a brief overview of tokens and embeddings in the context of deep learning text models, see <a href="https://msync.org/notes/llm-understanding-tokens-embeddings/">LLMs: Understanding Tokens and Embeddings</a></p>
<p>There can be - Standalone models just for embeddings (eg: Word2Vec <span class="citation" data-cites="mikolov2013efficientestimationwordrepresentations">(see <a href="#ref-mikolov2013efficientestimationwordrepresentations" role="doc-biblioref">Mikolov et al. 2013</a>)</span>, GloVe <span class="citation" data-cites="pennington2014glove">(see <a href="#ref-pennington2014glove" role="doc-biblioref">Pennington, Socher, and Manning 2014</a>)</span>), or - Models that use embeddings as part of a larger model (eg: BERT <span class="citation" data-cites="devlin2018bert">(see <a href="#ref-devlin2018bert" role="doc-biblioref">Devlin et al. 2018</a>)</span>, GPT-3 <span class="citation" data-cites="brown2020language">(see <a href="#ref-brown2020language" role="doc-biblioref">Brown et al. 2020</a>)</span>).</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-1-contents" aria-controls="callout-1" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-1" class="callout-1-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>Embeddings from one model are typically not directly compatible with another model, because the embeddings are learned in the context of the model.</p>
</div>
</div>
</div>
<section id="tokenization" class="level2" data-number="3.1">
<h2 data-number="3.1" class="anchored" data-anchor-id="tokenization"><span class="header-section-number">3.1</span> Tokenization</h2>
<p>The more tokens we have, the more information we can capture. However, more tokens also mean more computational resources.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-2-contents" aria-controls="callout-2" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
How does the token vocabulary affect computation?
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-2" class="callout-2-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<ul>
<li>The number of tokens affects the size of the embedding matrix.</li>
<li>The embedding matrix is a lookup table that maps each token to its corresponding embedding vector.</li>
<li>The size of the embedding matrix is determined by the vocabulary size and the embedding dimension. The larger the vocabulary, the larger the embedding matrix, and the more memory and computation are required to process it.</li>
</ul>
</div>
</div>
</div>
<ul>
<li>Tokens can be created not just for words, but also for subwords and characters.</li>
<li>This allows us to capture more information, and handle “words” not seen before.</li>
<li>We do not encode grammar rules (which are hard to define and may not be exhaustive or easy to update). Instead, we let the model learn the rules from the data. This is a key difference between traditional NLP and deep learning NLP.</li>
</ul>
<div id="d8492ba1" class="cell" data-execution_count="2">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1"></a><span class="im">import</span> importlib</span>
<span id="cb1-2"><a href="#cb1-2"></a><span class="im">import</span> stage1.tokenization</span>
<span id="cb1-3"><a href="#cb1-3"></a>importlib.<span class="bu">reload</span>(stage1.tokenization)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="1">
<pre><code>&lt;module 'stage1.tokenization' from '/Users/jaju/github/knowledge-garden/notes/llm-from-scratch/stage1/tokenization.py'&gt;</code></pre>
</div>
</div>
<section id="v1" class="level3" data-number="3.1.1">
<h3 data-number="3.1.1" class="anchored" data-anchor-id="v1"><span class="header-section-number">3.1.1</span> V1</h3>
<p>The first, naive implementation that can not handle unseen tokens, as tokens are identified based on word boundaries from the training text.</p>
<div id="2a8f3b01" class="cell" data-execution_count="3">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1"></a>stage1.tokenization.v1()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Error: Token 'draggees' not found in vocab.</code></pre>
</div>
</div>
</section>
<section id="v2" class="level3" data-number="3.1.2">
<h3 data-number="3.1.2" class="anchored" data-anchor-id="v2"><span class="header-section-number">3.1.2</span> V2</h3>
<p>Ability to handle unseen tokens, plus additional signals to like begin/end of text, padding, etc. We simply preprocess the text to handle unseen tokens and replace then with a special token.</p>
<div id="8069cb09" class="cell" data-execution_count="4">
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1"></a>stage1.tokenization.v2()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<div class="ansi-escaped-output">
<pre><span class="ansi-green-fg">2025-08-10 18:18:29.779</span> | <span class="ansi-bold">INFO    </span> | <span class="ansi-cyan-fg">utils.downloaders</span>:<span class="ansi-cyan-fg">download</span>:<span class="ansi-cyan-fg">21</span> - <span class="ansi-bold">data/the_verdict.txt already exists. Skipping download.</span>
</pre>
</div>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>╒═════════╤════════════════════════════╕
│ Input   │ three she draggees equally │
├─────────┼────────────────────────────┤
│ Encoded │ [1004, 876, 1131, 394]     │
├─────────┼────────────────────────────┤
│ Decoded │ three she &lt;|unk|&gt; equally  │
╘═════════╧════════════════════════════╛</code></pre>
</div>
</div>
</section>
<section id="v3---byte-pair-encoding-bpe" class="level3" data-number="3.1.3">
<h3 data-number="3.1.3" class="anchored" data-anchor-id="v3---byte-pair-encoding-bpe"><span class="header-section-number">3.1.3</span> V3 - Byte Pair Encoding (BPE)</h3>
<ul>
<li>Used in the original ChatGPT as well as GPT-2 and GPT-3.
<ul>
<li>This one goes further granular in how it identifies tokens - encompassing all atomic units and then certain agglomerations of them.- This is a data-driven approach to tokenization, where we learn the tokens from the data.</li>
</ul></li>
<li><a href="https://github.com/openai/tiktoken">tiktoken</a> is a Python implementation of BPE. Implementation from scratch is not a key aim of this book, but it is useful to understand the concepts.</li>
</ul>
<p>In this example, we don’t create a new vocabulary, but use the ‘GPT-2’ vocabulary. (See source)</p>
<div id="5124fdb2" class="cell" data-execution_count="5">
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1"></a>stage1.tokenization.v3()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<div class="ansi-escaped-output">
<pre><span class="ansi-green-fg">2025-08-10 18:18:29.785</span> | <span class="ansi-bold">INFO    </span> | <span class="ansi-cyan-fg">utils.downloaders</span>:<span class="ansi-cyan-fg">download</span>:<span class="ansi-cyan-fg">21</span> - <span class="ansi-bold">data/the_verdict.txt already exists. Skipping download.</span>
</pre>
</div>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>╒═════════╤════════════════════════════════════╕
│ Input   │ three she draggees equally         │
├─────────┼────────────────────────────────────┤
│ Encoded │ [15542, 673, 6715, 469, 274, 8603] │
├─────────┼────────────────────────────────────┤
│ Decoded │ three she draggees equally         │
╘═════════╧════════════════════════════════════╛</code></pre>
</div>
</div>
<p>Notice that the number of tokens is more than the number of distinct words in this example. We also have unseen words, which are split into multiple tokens that are in the vocabulary. This is a key feature of BPE, as it allows us to handle unseen words by breaking them down into smaller units that are in the vocabulary.</p>
</section>
<section id="v4" class="level3" data-number="3.1.4">
<h3 data-number="3.1.4" class="anchored" data-anchor-id="v4"><span class="header-section-number">3.1.4</span> V4</h3>
<p>This is a further improvement over the previous version. “Improvement” not because of the tokenization technique but because of how we inject special symbols into the text. We use a special token to indicate the beginning and end of a sentence, and a special token to indicate padding.</p>
<div id="05e12602" class="cell" data-execution_count="6">
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1"></a>stage1.tokenization.v4()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<div class="ansi-escaped-output">
<pre><span class="ansi-green-fg">2025-08-10 18:18:29.890</span> | <span class="ansi-bold">INFO    </span> | <span class="ansi-cyan-fg">utils.downloaders</span>:<span class="ansi-cyan-fg">download</span>:<span class="ansi-cyan-fg">21</span> - <span class="ansi-bold">data/the_verdict.txt already exists. Skipping download.</span>
</pre>
</div>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>╒═════════╤═══════════════════════════════════════════════════════════════════════════════╕
│ Input   │ three she dragged equally. &lt;|endoftext|&gt; This is the end of the document.     │
├─────────┼───────────────────────────────────────────────────────────────────────────────┤
│ Encoded │ [15542, 673, 17901, 8603, 13, 220, 50256, 770, 318, 262, 886, 286, 262, 3188, │
│         │ 13]                                                                           │
├─────────┼───────────────────────────────────────────────────────────────────────────────┤
│ Decoded │ three she dragged equally. &lt;|endoftext|&gt; This is the end of the document.     │
╘═════════╧═══════════════════════════════════════════════════════════════════════════════╛</code></pre>
</div>
</div>
<section id="bpe-a-further-demonstration" class="level4" data-number="3.1.4.1">
<h4 data-number="3.1.4.1" class="anchored" data-anchor-id="bpe-a-further-demonstration"><span class="header-section-number">3.1.4.1</span> BPE: A further demonstration</h4>
<p>BPE can handle (apparently) garbage text. And also, it preserves spaces between tokens, because it makes no special assumptions about the text, including the notion of word boundaries.</p>
<div id="4bbf71a0" class="cell" data-execution_count="7">
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1"></a>stage1.tokenization.v4bpe()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>╒═════════╤═════════════════════════════════════════════════════════════════════════════════╕
│ Input   │ asd asdjfkjsdf ksjfksa sdkfjsj   powiuosadoapofqfvv                             │
├─────────┼─────────────────────────────────────────────────────────────────────────────────┤
│ Encoded │ [292, 67, 355, 28241, 69, 74, 8457, 7568, 479, 82, 73, 69, 591, 64, 264, 34388, │
│         │ 69, 8457, 73, 220, 220, 7182, 16115, 418, 4533, 499, 1659, 80, 69, 25093]       │
├─────────┼─────────────────────────────────────────────────────────────────────────────────┤
│ Decoded │ asd asdjfkjsdf ksjfksa sdkfjsj   powiuosadoapofqfvv                             │
╘═════════╧═════════════════════════════════════════════════════════════════════════════════╛</code></pre>
</div>
</div>
</section>
</section>
</section>
<section id="embeddings" class="level2" data-number="3.2">
<h2 data-number="3.2" class="anchored" data-anchor-id="embeddings"><span class="header-section-number">3.2</span> Embeddings</h2>
<ul>
<li>Tokens as 1-hot vectors create sparse matrices that are inefficient and fail to capture semantic relationships</li>
<li>Embeddings represent tokens in a lower-dimensional space that preserves semantic relationships</li>
<li>Embeddings are learned, not universal representations, and are specific to the model they’re trained in</li>
<li>The embedding space dimension is a tunable hyperparameter (larger = more informative but more resource-intensive)</li>
<li>Embeddings start as random values and are refined during training through gradient updates</li>
</ul>
<div id="fcbf3911" class="cell" data-execution_count="8">
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1"></a><span class="im">import</span> importlib</span>
<span id="cb13-2"><a href="#cb13-2"></a><span class="im">import</span> stage1.embeddings</span>
<span id="cb13-3"><a href="#cb13-3"></a>importlib.<span class="bu">reload</span>(stage1.embeddings)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="7">
<pre><code>&lt;module 'stage1.embeddings' from '/Users/jaju/github/knowledge-garden/notes/llm-from-scratch/stage1/embeddings.py'&gt;</code></pre>
</div>
</div>
<section id="what-do-they-look-like" class="level3" data-number="3.2.1">
<h3 data-number="3.2.1" class="anchored" data-anchor-id="what-do-they-look-like"><span class="header-section-number">3.2.1</span> What do they look like?</h3>
<p>Let’s create a randomly initialized embedding matrix for a vocabulary of size 10 and an embedding dimension of 4. The embeddings matrix is then of size (10, 4), where each row corresponds to a token in the vocabulary and each column corresponds to a dimension in the embedding space.</p>
<div id="6a383193" class="cell" data-execution_count="9">
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1"></a><span class="im">import</span> torch</span>
<span id="cb15-2"><a href="#cb15-2"></a><span class="im">import</span> stage1.embeddings</span>
<span id="cb15-3"><a href="#cb15-3"></a>embedding_dim <span class="op">=</span> <span class="dv">4</span></span>
<span id="cb15-4"><a href="#cb15-4"></a>vocab_size <span class="op">=</span> <span class="dv">10</span></span>
<span id="cb15-5"><a href="#cb15-5"></a></span>
<span id="cb15-6"><a href="#cb15-6"></a>embeddings <span class="op">=</span> torch.nn.Embedding(vocab_size, embedding_dim)</span>
<span id="cb15-7"><a href="#cb15-7"></a><span class="bu">print</span>(embeddings.weight)</span>
<span id="cb15-8"><a href="#cb15-8"></a><span class="bu">print</span>(<span class="ss">f"Embedding of token-id 2 is </span><span class="sc">{</span>embeddings(torch.tensor([<span class="dv">1</span>]))<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Parameter containing:
tensor([[ 0.5681,  1.3156, -1.1681, -0.1468],
        [ 1.0959,  1.0497, -0.3855,  0.2652],
        [-0.8270,  0.1451, -0.6180, -2.1681],
        [ 0.3374,  0.5199, -0.4277,  0.3758],
        [-0.0170,  1.2219,  1.4865, -0.4767],
        [-0.5105, -1.1597,  1.5071,  0.8703],
        [ 1.0197, -0.4430, -1.1768, -0.7499],
        [-0.3121,  0.7563, -0.8407, -0.0094],
        [ 0.4255,  0.2792, -0.5903, -1.0489],
        [-1.8055,  0.6865,  0.0475,  1.1657]], requires_grad=True)
Embedding of token-id 2 is tensor([[ 1.0959,  1.0497, -0.3855,  0.2652]], grad_fn=&lt;EmbeddingBackward0&gt;)</code></pre>
</div>
</div>
<p>The embedding of a token with id <code>i</code> is simply the <code>i</code>-th row of the embedding matrix. For example, the embedding of the token with id 0 is the first row of the embedding matrix. Mapping tokens to embeddings is plainly a lookup operation.</p>
</section>
<section id="v0" class="level3" data-number="3.2.2">
<h3 data-number="3.2.2" class="anchored" data-anchor-id="v0"><span class="header-section-number">3.2.2</span> V0</h3>
<p>Printing the embedding matrix of a made-up vocabulary. The embeddings are randomly initialized. The embeddings matrix size is determined by the vocabulary size and the embedding dimension. The embedding dimension is a hyperparameter that can be tuned.</p>
<p>Embeddings are tensors with a shape of (vocab_size, embedding_dim).</p>
<div id="8d81ff20" class="cell" data-execution_count="10">
<div class="sourceCode cell-code" id="cb17"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb17-2"><a href="#cb17-2"></a>nn.Embedding(<span class="dv">10</span>, <span class="dv">4</span>).weight</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="9">
<pre><code>Parameter containing:
tensor([[-0.8666,  0.5709, -0.4242, -1.8706],
        [-1.1113,  1.7278, -1.1033,  0.1828],
        [-1.5127,  1.1119,  0.0240, -0.7175],
        [ 0.0837, -0.5080,  1.0393, -0.8134],
        [ 0.5476, -0.8740,  2.1083, -0.9301],
        [ 2.2264, -0.7628,  0.1427,  1.2580],
        [-0.1776,  0.8092, -1.5062,  2.8540],
        [-0.6710, -2.3110, -2.9075, -0.2702],
        [ 0.1821,  1.3021, -0.4444,  0.7293],
        [ 0.2201, -0.6899, -0.0610,  2.0738]], requires_grad=True)</code></pre>
</div>
</div>
<p>Once we have the embeddings matrix, we can map input tokens to their corresponding embeddings.</p>
<div id="4bba82d3" class="cell" data-execution_count="11">
<div class="sourceCode cell-code" id="cb19"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1"></a>stage1.embeddings.v0()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Pseudo-randomly initialized embedding layer:
╒══════════════════════════╤══════════════════════════════════════════════════════════════════════════════╕
│ Vocab Size               │ 10                                                                           │
├──────────────────────────┼──────────────────────────────────────────────────────────────────────────────┤
│ Output Dimension         │ 3                                                                            │
├──────────────────────────┼──────────────────────────────────────────────────────────────────────────────┤
│ Embedding Layer          │ Embedding(10, 3)                                                             │
├──────────────────────────┼──────────────────────────────────────────────────────────────────────────────┤
│ Layer Weights            │ Parameter containing: tensor([[-1.1258, -1.1524, -0.2506],         [-0.4339, │
│                          │ 0.8487,  0.6920],         [-0.3160, -2.1152,  0.3223],         [-1.2633,     │
│                          │ 0.3500,  0.3081],         [ 0.1198,  1.2377, -0.1435],         [-0.1116,     │
│                          │ -0.6136,  0.0316],         [-0.4927,  0.2484,  0.4397],         [ 0.1124,    │
│                          │ -0.8411, -2.3160],         [-0.1023,  0.7924, -0.2897],         [ 0.0525,    │
│                          │ 0.5229,  2.3022]], requires_grad=True)                                       │
├──────────────────────────┼──────────────────────────────────────────────────────────────────────────────┤
│ Embedding for token_id 5 │ tensor([-0.1116, -0.6136,  0.0316], grad_fn=&lt;EmbeddingBackward0&gt;)            │
╘══════════════════════════╧══════════════════════════════════════════════════════════════════════════════╛
╒═══════════════════════════════════╤═══════════════════════════════════════════════════════════════════════════╕
│ Embedding for token_ids [2, 4, 6] │ tensor([[-0.3160, -2.1152,  0.3223],         [ 0.1198,  1.2377, -0.1435], │
│                                   │ [-0.4927,  0.2484,  0.4397]], grad_fn=&lt;EmbeddingBackward0&gt;)               │
╘═══════════════════════════════════╧═══════════════════════════════════════════════════════════════════════════╛</code></pre>
</div>
</div>
</section>
<section id="v1-1" class="level3" data-number="3.2.3">
<h3 data-number="3.2.3" class="anchored" data-anchor-id="v1-1"><span class="header-section-number">3.2.3</span> V1</h3>
<ul>
<li>Data loaders create batches of token-ids for efficient processing</li>
<li>Batching is valuable for large text corpora</li>
<li>Batched processing leverages GPU acceleration when available</li>
<li>Even on CPU, batching enables more efficient multi-threaded processing</li>
<li>The example demonstrates creating embeddings for “The Verdict” text using previously created data loader with batch processing</li>
</ul>
<div id="0ba8b924" class="cell" data-execution_count="12">
<div class="sourceCode cell-code" id="cb21"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1"></a>stage1.embeddings.v1()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>╒════════════════╤═══════╕
│ vocab_size     │ 50257 │
├────────────────┼───────┤
│ embedding_size │   256 │
├────────────────┼───────┤
│ input_length   │     4 │
├────────────────┼───────┤
│ batch_size     │     8 │
╘════════════════╧═══════╛
Embedding Layer: Parameter containing:
tensor([[ 0.9383,  0.4889, -0.6731,  ...,  1.2948,  1.4628, -0.6204],
        [ 0.6257, -1.2231, -0.6232,  ...,  0.3260,  0.5352,  1.9733],
        [-1.4115, -1.0295,  0.1267,  ...,  0.5027, -0.8871,  1.9974],
        ...,
        [ 0.6928, -0.5382, -0.8726,  ..., -0.5148,  0.9695,  0.7689],
        [-0.5866,  0.6971,  1.8386,  ...,  0.4298, -0.5139,  1.6624],
        [ 0.6073,  0.2991,  0.7669,  ..., -1.3811, -1.4284, -0.5630]],
       requires_grad=True)
Inputs shape: torch.Size([8, 4])
Targets shape: torch.Size([8, 4])
Input tensor shape:  torch.Size([1, 4])
Input Tensor: tensor([[39936, 24254,  7996, 42174]])
Output tensor shape:  torch.Size([1, 4, 256])
Output Tensor: tensor([[[-0.9905,  0.4149, -0.1217,  ...,  2.3362, -0.5502,  0.3072],
         [ 2.0188,  0.2669, -0.0151,  ..., -0.1302,  0.0308, -0.0452],
         [ 1.7518,  1.2162, -1.0058,  ..., -1.2053, -0.8477, -0.3506],
         [ 0.3572, -1.6816,  1.1135,  ...,  0.8234,  0.5311, -0.3427]]],
       grad_fn=&lt;EmbeddingBackward0&gt;)</code></pre>
</div>
</div>
</section>
<section id="v2---using-word-positions" class="level3" data-number="3.2.4">
<h3 data-number="3.2.4" class="anchored" data-anchor-id="v2---using-word-positions"><span class="header-section-number">3.2.4</span> V2 - Using word positions</h3>
<ul>
<li>Taking the previous example and thinking further, while embeddings capture semantic relationships in a denser space, we also need to encode token positions in sentences.
<ul>
<li>A word’s meaning can change based on its position</li>
<li>A sentence’s meaning can change based on word order</li>
<li>We hypothesize this and hope our neural network architecture will learn it</li>
</ul></li>
<li>We’ll abstract away the details of position encoding implementation:
<ul>
<li>No clean notion of position exists (no defined start/end for text we process)</li>
<li>Focus on the current input and calculations relative to it</li>
</ul></li>
<li>Even within the current input batch, position is a hazy concept due to sliding windows:
<ul>
<li>Options include using absolute positions within current sequence/batch</li>
<li>Or encoding relative positions instead</li>
</ul></li>
</ul>
<div id="8fde735a" class="cell" data-execution_count="13">
<div class="sourceCode cell-code" id="cb23"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb23-1"><a href="#cb23-1"></a>stage1.embeddings.v2pos()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>╒════════════════╤═══════╕
│ vocab_size     │ 50257 │
├────────────────┼───────┤
│ embedding_size │   256 │
├────────────────┼───────┤
│ input_length   │     4 │
├────────────────┼───────┤
│ batch_size     │     8 │
╘════════════════╧═══════╛
Embedding Layer: Parameter containing:
tensor([[ 0.7480,  2.0246,  0.8172,  ...,  1.4958,  0.9920, -0.4635],
        [-0.0083,  2.1477, -0.6221,  ..., -0.6456, -0.7785, -0.0448],
        [ 0.3099,  0.7856,  1.1800,  ...,  0.0087, -0.3606, -1.5200],
        ...,
        [ 0.6470,  1.1495,  1.0626,  ...,  1.2715, -0.1108, -0.4586],
        [ 0.0704, -1.2130, -1.3772,  ..., -1.3077,  0.3081,  0.8306],
        [ 0.3290,  0.2603, -1.1474,  ..., -0.3902, -0.7557, -0.1635]],
       requires_grad=True)
Positional Embedding Layer: Parameter containing:
tensor([[-1.2746,  2.2625, -2.0476,  ..., -0.0991, -0.1806,  0.0748],
        [ 1.5418, -0.9089, -0.5957,  ..., -0.9257,  1.2022,  0.9211],
        [-0.4068, -0.5110,  0.1881,  ..., -0.0294, -0.7648, -1.0097],
        [ 1.6700, -1.1958, -1.2150,  ...,  0.0909,  0.1570, -2.0347]],
       requires_grad=True)
Input shape:  torch.Size([8, 4, 256])
Plain token embeddings:  tensor([[[ 0.4318,  0.5522,  0.9293,  ...,  0.8221,  0.5407, -0.2860],
         [-1.2446, -0.1105,  0.8851,  ..., -0.6168, -0.1016,  0.8931],
         [-1.2060, -1.4052,  0.7802,  ...,  0.0574,  0.4872,  1.6930],
         [ 1.4304, -1.1654, -0.3654,  ...,  2.6416, -1.2475,  0.5927]],

        [[-0.2098, -2.4730,  0.5364,  ...,  1.1667, -0.3026, -0.9157],
         [-1.2725, -1.0119, -0.2631,  ...,  0.9508,  0.6296,  0.0675],
         [-2.2059, -1.3065, -0.8812,  ...,  1.1935, -0.9095, -0.8640],
         [ 0.6062,  0.6820, -0.1552,  ..., -3.4933, -1.6555, -0.2279]],

        [[ 0.7390, -1.6588,  1.5115,  ...,  0.3389, -0.9614, -0.7456],
         [ 0.5239, -0.7570, -0.1204,  ..., -0.2197, -1.0866, -0.7499],
         [-1.2727,  0.1583, -2.1473,  ..., -0.9593, -0.7086, -0.1224],
         [ 0.7573, -0.7136,  0.9684,  ..., -0.1977, -0.0050,  1.3835]],

        ...,

        [[-0.0757, -0.3070, -0.9718,  ..., -0.8089,  0.4655,  1.1395],
         [ 0.1489,  0.1279, -0.8199,  ...,  0.1329, -0.4307,  0.8254],
         [ 0.2233,  0.7468, -0.3207,  ..., -0.9275,  2.2948,  1.0906],
         [-0.1399, -0.3639, -1.8264,  ...,  0.3288, -0.4609,  0.9846]],

        [[-1.2727,  0.1583, -2.1473,  ..., -0.9593, -0.7086, -0.1224],
         [-1.2725, -1.0119, -0.2631,  ...,  0.9508,  0.6296,  0.0675],
         [ 1.3725, -1.0106, -0.0334,  ..., -0.9763,  0.0051,  0.5848],
         [ 1.3725, -1.0106, -0.0334,  ..., -0.9763,  0.0051,  0.5848]],

        [[-1.2725, -1.0119, -0.2631,  ...,  0.9508,  0.6296,  0.0675],
         [ 1.3725, -1.0106, -0.0334,  ..., -0.9763,  0.0051,  0.5848],
         [ 1.3725, -1.0106, -0.0334,  ..., -0.9763,  0.0051,  0.5848],
         [ 1.3725, -1.0106, -0.0334,  ..., -0.9763,  0.0051,  0.5848]]],
       grad_fn=&lt;EmbeddingBackward0&gt;)
Positional embeddings:  tensor([[[-0.8428,  2.8148, -1.1183,  ...,  0.7230,  0.3600, -0.2112],
         [ 0.2972, -1.0194,  0.2894,  ..., -1.5425,  1.1006,  1.8142],
         [-1.6128, -1.9162,  0.9683,  ...,  0.0281, -0.2776,  0.6833],
         [ 3.1005, -2.3613, -1.5804,  ...,  2.7324, -1.0906, -1.4421]],

        [[-1.4844, -0.2104, -1.5112,  ...,  1.0675, -0.4832, -0.8409],
         [ 0.2693, -1.9208, -0.8589,  ...,  0.0251,  1.8318,  0.9886],
         [-2.6128, -1.8175, -0.6931,  ...,  1.1642, -1.6742, -1.8737],
         [ 2.2762, -0.5139, -1.3702,  ..., -3.4024, -1.4985, -2.2626]],

        [[-0.5357,  0.6037, -0.5361,  ...,  0.2398, -1.1420, -0.6708],
         [ 2.0657, -1.6659, -0.7162,  ..., -1.1454,  0.1156,  0.1713],
         [-1.6795, -0.3528, -1.9592,  ..., -0.9887, -1.4734, -1.1320],
         [ 2.4273, -1.9094, -0.2466,  ..., -0.1069,  0.1520, -0.6512]],

        ...,

        [[-1.3503,  1.9556, -3.0194,  ..., -0.9080,  0.2848,  1.2143],
         [ 1.6907, -0.7810, -1.4156,  ..., -0.7928,  0.7715,  1.7465],
         [-0.1835,  0.2357, -0.1326,  ..., -0.9569,  1.5301,  0.0809],
         [ 1.5301, -1.5597, -3.0414,  ...,  0.4197, -0.3039, -1.0502]],

        [[-2.5473,  2.4208, -4.1948,  ..., -1.0584, -0.8893, -0.0476],
         [ 0.2693, -1.9208, -0.8589,  ...,  0.0251,  1.8318,  0.9886],
         [ 0.9656, -1.5217,  0.1547,  ..., -1.0056, -0.7597, -0.4248],
         [ 3.0425, -2.2065, -1.2484,  ..., -0.8854,  0.1621, -1.4499]],

        [[-2.5472,  1.2507, -2.3107,  ...,  0.8517,  0.4489,  0.1423],
         [ 2.9143, -1.9195, -0.6292,  ..., -1.9020,  1.2073,  1.5059],
         [ 0.9656, -1.5217,  0.1547,  ..., -1.0056, -0.7597, -0.4248],
         [ 3.0425, -2.2065, -1.2484,  ..., -0.8854,  0.1621, -1.4499]]],
       grad_fn=&lt;AddBackward0&gt;)</code></pre>
</div>
</div>


<!-- -->


</section>
</section>
</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" role="doc-bibliography" id="quarto-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list">
<div id="ref-brown2020language" class="csl-entry" role="listitem">
Brown, Tom B., Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, et al. 2020. <span>“Language Models Are Few-Shot Learners.”</span> <a href="https://arxiv.org/abs/2005.14165">https://arxiv.org/abs/2005.14165</a>.
</div>
<div id="ref-devlin2018bert" class="csl-entry" role="listitem">
Devlin, Jacob, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. <span>“BERT: Pre-Training of Deep Bidirectional Transformers for Language Understanding.”</span> <a href="https://arxiv.org/abs/1810.04805">https://arxiv.org/abs/1810.04805</a>.
</div>
<div id="ref-gpt3technicaloverview" class="csl-entry" role="listitem">
Li, Chuan. 2020. <span>“GPT-3: A Technical Overview.”</span> <a href="https://lambdalabs.com/blog/demystifying-gpt-3">https://lambdalabs.com/blog/demystifying-gpt-3</a>.
</div>
<div id="ref-mikolov2013efficientestimationwordrepresentations" class="csl-entry" role="listitem">
Mikolov, Tomas, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013. <span>“Efficient Estimation of Word Representations in Vector Space.”</span> <a href="https://arxiv.org/abs/1301.3781">https://arxiv.org/abs/1301.3781</a>.
</div>
<div id="ref-instruct-gpt" class="csl-entry" role="listitem">
Ouyang, Long, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, et al. 2022. <span>“Training Language Models to Follow Instructions with Human Feedback.”</span> <a href="https://arxiv.org/abs/2203.02155">https://arxiv.org/abs/2203.02155</a>.
</div>
<div id="ref-pennington2014glove" class="csl-entry" role="listitem">
Pennington, Jeffrey, Richard Socher, and Christopher D. Manning. 2014. <span>“GloVe: Global Vectors for Word Representation.”</span> In <em>Empirical Methods in Natural Language Processing (EMNLP)</em>, 1532–43. <a href="http://www.aclweb.org/anthology/D14-1162">http://www.aclweb.org/anthology/D14-1162</a>.
</div>
<div id="ref-dolma2024" class="csl-entry" role="listitem">
Soldaini, Luca, Rodney Kinney, Akshita Bhagia, Dustin Schwenk, David Atkinson, Russell Authur, Ben Bogin, et al. 2024. <span>“Dolma: An Open Corpus of Three Trillion Tokens for Language Model Pretraining Research.”</span> <a href="https://arxiv.org/abs/2402.00159">https://arxiv.org/abs/2402.00159</a>.
</div>
</div></section></div></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
        const codeEl = trigger.previousElementSibling.cloneNode(true);
        for (const childEl of codeEl.children) {
          if (isCodeAnnotation(childEl)) {
            childEl.remove();
          }
        }
        return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
    const viewSource = window.document.getElementById('quarto-view-source') ||
                       window.document.getElementById('quarto-code-tools-source');
    if (viewSource) {
      const sourceUrl = viewSource.getAttribute("data-quarto-source-url");
      viewSource.addEventListener("click", function(e) {
        if (sourceUrl) {
          // rstudio viewer pane
          if (/\bcapabilities=\b/.test(window.location)) {
            window.open(sourceUrl);
          } else {
            window.location.href = sourceUrl;
          }
        } else {
          const modal = new bootstrap.Modal(document.getElementById('quarto-embedded-source-code-modal'));
          modal.show();
        }
        return false;
      });
    }
    function toggleCodeHandler(show) {
      return function(e) {
        const detailsSrc = window.document.querySelectorAll(".cell > details > .sourceCode");
        for (let i=0; i<detailsSrc.length; i++) {
          const details = detailsSrc[i].parentElement;
          if (show) {
            details.open = true;
          } else {
            details.removeAttribute("open");
          }
        }
        const cellCodeDivs = window.document.querySelectorAll(".cell > .sourceCode");
        const fromCls = show ? "hidden" : "unhidden";
        const toCls = show ? "unhidden" : "hidden";
        for (let i=0; i<cellCodeDivs.length; i++) {
          const codeDiv = cellCodeDivs[i];
          if (codeDiv.classList.contains(fromCls)) {
            codeDiv.classList.remove(fromCls);
            codeDiv.classList.add(toCls);
          } 
        }
        return false;
      }
    }
    const hideAllCode = window.document.getElementById("quarto-hide-all-code");
    if (hideAllCode) {
      hideAllCode.addEventListener("click", toggleCodeHandler(false));
    }
    const showAllCode = window.document.getElementById("quarto-show-all-code");
    if (showAllCode) {
      showAllCode.addEventListener("click", toggleCodeHandler(true));
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp("https:\/\/jaju\.github\.io\/knowledge-garden\/");
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script><div class="modal fade" id="quarto-embedded-source-code-modal" tabindex="-1" aria-labelledby="quarto-embedded-source-code-modal-label" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable"><div class="modal-content"><div class="modal-header"><h5 class="modal-title" id="quarto-embedded-source-code-modal-label">Source Code</h5><button class="btn-close" data-bs-dismiss="modal"></button></div><div class="modal-body"><div class="">
<div class="sourceCode" id="cb25" data-shortcodes="false"><pre class="sourceCode numberSource markdown number-lines code-with-copy"><code class="sourceCode markdown"><span id="cb25-1"><a href="#cb25-1"></a><span class="co">---</span></span>
<span id="cb25-2"><a href="#cb25-2"></a><span class="an">title:</span><span class="co"> "Notes: LLM From Scratch"</span></span>
<span id="cb25-3"><a href="#cb25-3"></a><span class="an">number-sections:</span><span class="co"> true</span></span>
<span id="cb25-4"><a href="#cb25-4"></a><span class="an">bibliography:</span><span class="co"> references.bib</span></span>
<span id="cb25-5"><a href="#cb25-5"></a><span class="an">engine:</span><span class="co"> jupyter</span></span>
<span id="cb25-6"><a href="#cb25-6"></a><span class="an">jupyter:</span><span class="co"> kg-llm-from-scratch</span></span>
<span id="cb25-7"><a href="#cb25-7"></a><span class="an">execute:</span></span>
<span id="cb25-8"><a href="#cb25-8"></a><span class="co">  daemon: false</span></span>
<span id="cb25-9"><a href="#cb25-9"></a><span class="co">  cache: true</span></span>
<span id="cb25-10"><a href="#cb25-10"></a><span class="co">  freeze: auto</span></span>
<span id="cb25-11"><a href="#cb25-11"></a><span class="an">format:</span></span>
<span id="cb25-12"><a href="#cb25-12"></a><span class="co">  html:</span></span>
<span id="cb25-13"><a href="#cb25-13"></a><span class="co">    toc: true</span></span>
<span id="cb25-14"><a href="#cb25-14"></a><span class="co">    code-fold: false</span></span>
<span id="cb25-15"><a href="#cb25-15"></a><span class="co">    code-summary: "Click to expand/collapse"</span></span>
<span id="cb25-16"><a href="#cb25-16"></a><span class="co">    code-tools: true</span></span>
<span id="cb25-17"><a href="#cb25-17"></a><span class="co">    code-line-numbers: true</span></span>
<span id="cb25-18"><a href="#cb25-18"></a><span class="co">    css: styles.css</span></span>
<span id="cb25-19"><a href="#cb25-19"></a><span class="co">---</span></span>
<span id="cb25-20"><a href="#cb25-20"></a></span>
<span id="cb25-21"><a href="#cb25-21"></a><span class="fu"># Synopsis</span></span>
<span id="cb25-22"><a href="#cb25-22"></a></span>
<span id="cb25-23"><a href="#cb25-23"></a>Notes and pointers during the exploration of the book <span class="co">[</span><span class="ot">LLM From Scratch</span><span class="co">](https://www.manning.com/books/build-a-large-language-model-from-scratch)</span> by <span class="co">[</span><span class="ot">Sebastian Raschka</span><span class="co">](https://github.com/rasbt)</span>. While I am using this book as the central reference, the notes will contain additional references and pointers to other resources.</span>
<span id="cb25-24"><a href="#cb25-24"></a></span>
<span id="cb25-25"><a href="#cb25-25"></a><span class="fu"># Chapter 1: Understanding Large Language Models {#sec-chapter-1}</span></span>
<span id="cb25-26"><a href="#cb25-26"></a></span>
<span id="cb25-27"><a href="#cb25-27"></a><span class="ss">- </span>A good quality corpus is crucial.</span>
<span id="cb25-28"><a href="#cb25-28"></a><span class="ss">- </span>Choice of corpus will depend on the downstream tasks.</span>
<span id="cb25-29"><a href="#cb25-29"></a><span class="ss">- </span>For example, to generate code, the corpus should contain a substantial amount of code snippets.</span>
<span id="cb25-30"><a href="#cb25-30"></a></span>
<span id="cb25-31"><a href="#cb25-31"></a>An open-source corpus of 3 trillion tokens has been detailed in <span class="co">[</span><span class="ot">@dolma2024</span><span class="co">]</span>.</span>
<span id="cb25-32"><a href="#cb25-32"></a></span>
<span id="cb25-33"><a href="#cb25-33"></a><span class="fu">## Key challenges in pre-training</span></span>
<span id="cb25-34"><a href="#cb25-34"></a><span class="ss">  - </span>Data: Creating or curating a large corpus of text data is a significant challenge. The quality and diversity of the data are crucial for the performance of the model.</span>
<span id="cb25-35"><a href="#cb25-35"></a><span class="ss">  - </span>Model: The model architecture is key for pre-training. A good model architecture will help in learning the underlying structure of the data.</span>
<span id="cb25-36"><a href="#cb25-36"></a><span class="ss">  - </span>Budget: Pre-training a large language model requires a lot of computational resources. A good budget is crucial for pre-training.</span>
<span id="cb25-37"><a href="#cb25-37"></a><span class="ss">  - </span><span class="co">[</span><span class="ot">GPT-3</span><span class="co">](https://en.wikipedia.org/wiki/GPT-3)</span> potentially cost $4.6 million to train, per @gpt3technicaloverview.</span>
<span id="cb25-38"><a href="#cb25-38"></a></span>
<span id="cb25-39"><a href="#cb25-39"></a>@instruct-gpt presented ideas on how to fine-tune GPT-3 on a dataset of instructions.</span>
<span id="cb25-40"><a href="#cb25-40"></a></span>
<span id="cb25-41"><a href="#cb25-41"></a><span class="fu"># Chapter 2: Working with text data</span></span>
<span id="cb25-42"><a href="#cb25-42"></a></span>
<span id="cb25-43"><a href="#cb25-43"></a>For a brief overview of tokens and embeddings in the context of deep learning text models, see <span class="co">[</span><span class="ot">LLMs: Understanding Tokens and Embeddings</span><span class="co">](https://msync.org/notes/llm-understanding-tokens-embeddings/)</span></span>
<span id="cb25-44"><a href="#cb25-44"></a></span>
<span id="cb25-45"><a href="#cb25-45"></a>There can be </span>
<span id="cb25-46"><a href="#cb25-46"></a><span class="ss">- </span>Standalone models just for embeddings (eg: Word2Vec <span class="co">[</span><span class="ot">see @mikolov2013efficientestimationwordrepresentations</span><span class="co">]</span>, GloVe <span class="co">[</span><span class="ot">see @pennington2014glove</span><span class="co">]</span>), or</span>
<span id="cb25-47"><a href="#cb25-47"></a><span class="ss">- </span>Models that use embeddings as part of a larger model (eg: BERT <span class="co">[</span><span class="ot">see @devlin2018bert</span><span class="co">]</span>, GPT-3 <span class="co">[</span><span class="ot">see @brown2020language</span><span class="co">]</span>).</span>
<span id="cb25-48"><a href="#cb25-48"></a></span>
<span id="cb25-49"><a href="#cb25-49"></a>::: {.callout-note collapse="true"}</span>
<span id="cb25-50"><a href="#cb25-50"></a>Embeddings from one model are typically not directly compatible with another model, because the embeddings are learned in the context of the model.</span>
<span id="cb25-51"><a href="#cb25-51"></a>:::</span>
<span id="cb25-52"><a href="#cb25-52"></a></span>
<span id="cb25-53"><a href="#cb25-53"></a><span class="fu">## Tokenization</span></span>
<span id="cb25-54"><a href="#cb25-54"></a>The more tokens we have, the more information we can capture. However, more tokens also mean more computational resources.</span>
<span id="cb25-55"><a href="#cb25-55"></a></span>
<span id="cb25-56"><a href="#cb25-56"></a></span>
<span id="cb25-57"><a href="#cb25-57"></a>::: {.callout-note collapse="true"}</span>
<span id="cb25-58"><a href="#cb25-58"></a><span class="fu">### How does the token vocabulary affect computation?</span></span>
<span id="cb25-59"><a href="#cb25-59"></a><span class="ss">- </span>The number of tokens affects the size of the embedding matrix.</span>
<span id="cb25-60"><a href="#cb25-60"></a><span class="ss">- </span>The embedding matrix is a lookup table that maps each token to its corresponding embedding vector.</span>
<span id="cb25-61"><a href="#cb25-61"></a><span class="ss">- </span>The size of the embedding matrix is determined by the vocabulary size and the embedding dimension. The larger the vocabulary, the larger the embedding matrix, and the more memory and computation are required to process it.</span>
<span id="cb25-62"><a href="#cb25-62"></a>:::</span>
<span id="cb25-63"><a href="#cb25-63"></a></span>
<span id="cb25-64"><a href="#cb25-64"></a><span class="ss">- </span>Tokens can be created not just for words, but also for subwords and characters.</span>
<span id="cb25-65"><a href="#cb25-65"></a><span class="ss">- </span>This allows us to capture more information, and handle "words" not seen before.</span>
<span id="cb25-66"><a href="#cb25-66"></a><span class="ss">- </span>We do not encode grammar rules (which are hard to define and may not be exhaustive or easy to update). Instead, we let the model learn the rules from the data. This is a key difference between traditional NLP and deep learning NLP.</span>
<span id="cb25-67"><a href="#cb25-67"></a></span>
<span id="cb25-68"><a href="#cb25-68"></a></span>
<span id="cb25-71"><a href="#cb25-71"></a><span class="in">```{python}</span></span>
<span id="cb25-72"><a href="#cb25-72"></a><span class="im">import</span> importlib</span>
<span id="cb25-73"><a href="#cb25-73"></a><span class="im">import</span> stage1.tokenization</span>
<span id="cb25-74"><a href="#cb25-74"></a>importlib.<span class="bu">reload</span>(stage1.tokenization)</span>
<span id="cb25-75"><a href="#cb25-75"></a><span class="in">```</span></span>
<span id="cb25-76"><a href="#cb25-76"></a></span>
<span id="cb25-77"><a href="#cb25-77"></a><span class="fu">### V1</span></span>
<span id="cb25-78"><a href="#cb25-78"></a>The first, naive implementation that can not handle unseen tokens, as tokens are identified based on word boundaries from the training text.</span>
<span id="cb25-79"><a href="#cb25-79"></a></span>
<span id="cb25-82"><a href="#cb25-82"></a><span class="in">```{python}</span></span>
<span id="cb25-83"><a href="#cb25-83"></a>stage1.tokenization.v1()</span>
<span id="cb25-84"><a href="#cb25-84"></a><span class="in">```</span></span>
<span id="cb25-85"><a href="#cb25-85"></a></span>
<span id="cb25-86"><a href="#cb25-86"></a></span>
<span id="cb25-87"><a href="#cb25-87"></a><span class="fu">### V2</span></span>
<span id="cb25-88"><a href="#cb25-88"></a>Ability to handle unseen tokens, plus additional signals to like begin/end of text, padding, etc. We simply preprocess the text to handle unseen tokens and replace then with a special token.</span>
<span id="cb25-89"><a href="#cb25-89"></a></span>
<span id="cb25-92"><a href="#cb25-92"></a><span class="in">```{python}</span></span>
<span id="cb25-93"><a href="#cb25-93"></a>stage1.tokenization.v2()</span>
<span id="cb25-94"><a href="#cb25-94"></a><span class="in">```</span></span>
<span id="cb25-95"><a href="#cb25-95"></a></span>
<span id="cb25-96"><a href="#cb25-96"></a><span class="fu">### V3 - Byte Pair Encoding (BPE)</span></span>
<span id="cb25-97"><a href="#cb25-97"></a><span class="ss">- </span>Used in the original ChatGPT as well as GPT-2 and GPT-3. </span>
<span id="cb25-98"><a href="#cb25-98"></a><span class="ss">  - </span>This one goes further granular in how it identifies tokens - encompassing all atomic units and then certain agglomerations of them.- This is a data-driven approach to tokenization, where we learn the tokens from the data.</span>
<span id="cb25-99"><a href="#cb25-99"></a><span class="ss">- </span><span class="co">[</span><span class="ot">tiktoken</span><span class="co">](https://github.com/openai/tiktoken)</span> is a Python implementation of BPE. Implementation from scratch is not a key aim of this book, but it is useful to understand the concepts.</span>
<span id="cb25-100"><a href="#cb25-100"></a></span>
<span id="cb25-101"><a href="#cb25-101"></a>In this example, we don't create a new vocabulary, but use the 'GPT-2' vocabulary. (See source)</span>
<span id="cb25-102"><a href="#cb25-102"></a></span>
<span id="cb25-105"><a href="#cb25-105"></a><span class="in">```{python}</span></span>
<span id="cb25-106"><a href="#cb25-106"></a>stage1.tokenization.v3()</span>
<span id="cb25-107"><a href="#cb25-107"></a><span class="in">```</span></span>
<span id="cb25-108"><a href="#cb25-108"></a>Notice that the number of tokens is more than the number of distinct words in this example. We also have unseen words, which are split into multiple tokens that are in the vocabulary. This is a key feature of BPE, as it allows us to handle unseen words by breaking them down into smaller units that are in the vocabulary.</span>
<span id="cb25-109"><a href="#cb25-109"></a></span>
<span id="cb25-110"><a href="#cb25-110"></a><span class="fu">### V4 </span></span>
<span id="cb25-111"><a href="#cb25-111"></a>This is a further improvement over the previous version. "Improvement" not because of the tokenization technique but because of how we inject special symbols into the text. We use a special token to indicate the beginning and end of a sentence, and a special token to indicate padding.</span>
<span id="cb25-112"><a href="#cb25-112"></a></span>
<span id="cb25-115"><a href="#cb25-115"></a><span class="in">```{python}</span></span>
<span id="cb25-116"><a href="#cb25-116"></a>stage1.tokenization.v4()</span>
<span id="cb25-117"><a href="#cb25-117"></a><span class="in">```</span></span>
<span id="cb25-118"><a href="#cb25-118"></a></span>
<span id="cb25-119"><a href="#cb25-119"></a><span class="fu">#### BPE: A further demonstration</span></span>
<span id="cb25-120"><a href="#cb25-120"></a>BPE can handle (apparently) garbage text. And also, it preserves spaces between tokens, because it makes no special assumptions about the text, including the notion of word boundaries.</span>
<span id="cb25-121"><a href="#cb25-121"></a></span>
<span id="cb25-124"><a href="#cb25-124"></a><span class="in">```{python}</span></span>
<span id="cb25-125"><a href="#cb25-125"></a>stage1.tokenization.v4bpe()</span>
<span id="cb25-126"><a href="#cb25-126"></a><span class="in">```</span></span>
<span id="cb25-127"><a href="#cb25-127"></a></span>
<span id="cb25-128"><a href="#cb25-128"></a><span class="fu">## Embeddings</span></span>
<span id="cb25-129"><a href="#cb25-129"></a></span>
<span id="cb25-130"><a href="#cb25-130"></a><span class="ss">- </span>Tokens as 1-hot vectors create sparse matrices that are inefficient and fail to capture semantic relationships</span>
<span id="cb25-131"><a href="#cb25-131"></a><span class="ss">- </span>Embeddings represent tokens in a lower-dimensional space that preserves semantic relationships</span>
<span id="cb25-132"><a href="#cb25-132"></a><span class="ss">- </span>Embeddings are learned, not universal representations, and are specific to the model they're trained in</span>
<span id="cb25-133"><a href="#cb25-133"></a><span class="ss">- </span>The embedding space dimension is a tunable hyperparameter (larger = more informative but more resource-intensive)</span>
<span id="cb25-134"><a href="#cb25-134"></a><span class="ss">- </span>Embeddings start as random values and are refined during training through gradient updates</span>
<span id="cb25-135"><a href="#cb25-135"></a></span>
<span id="cb25-138"><a href="#cb25-138"></a><span class="in">```{python}</span></span>
<span id="cb25-139"><a href="#cb25-139"></a><span class="im">import</span> importlib</span>
<span id="cb25-140"><a href="#cb25-140"></a><span class="im">import</span> stage1.embeddings</span>
<span id="cb25-141"><a href="#cb25-141"></a>importlib.<span class="bu">reload</span>(stage1.embeddings)</span>
<span id="cb25-142"><a href="#cb25-142"></a><span class="in">```</span></span>
<span id="cb25-143"><a href="#cb25-143"></a></span>
<span id="cb25-144"><a href="#cb25-144"></a><span class="fu">### What do they look like?</span></span>
<span id="cb25-145"><a href="#cb25-145"></a>Let's create a randomly initialized embedding matrix for a vocabulary of size 10 and an embedding dimension of 4. The embeddings matrix is then of size (10, 4), where each row corresponds to a token in the vocabulary and each column corresponds to a dimension in the embedding space.</span>
<span id="cb25-148"><a href="#cb25-148"></a><span class="in">```{python}</span></span>
<span id="cb25-149"><a href="#cb25-149"></a><span class="im">import</span> torch</span>
<span id="cb25-150"><a href="#cb25-150"></a><span class="im">import</span> stage1.embeddings</span>
<span id="cb25-151"><a href="#cb25-151"></a>embedding_dim <span class="op">=</span> <span class="dv">4</span></span>
<span id="cb25-152"><a href="#cb25-152"></a>vocab_size <span class="op">=</span> <span class="dv">10</span></span>
<span id="cb25-153"><a href="#cb25-153"></a></span>
<span id="cb25-154"><a href="#cb25-154"></a>embeddings <span class="op">=</span> torch.nn.Embedding(vocab_size, embedding_dim)</span>
<span id="cb25-155"><a href="#cb25-155"></a><span class="bu">print</span>(embeddings.weight)</span>
<span id="cb25-156"><a href="#cb25-156"></a><span class="bu">print</span>(<span class="ss">f"Embedding of token-id 2 is </span><span class="sc">{</span>embeddings(torch.tensor([<span class="dv">1</span>]))<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb25-157"><a href="#cb25-157"></a><span class="in">```</span></span>
<span id="cb25-158"><a href="#cb25-158"></a></span>
<span id="cb25-159"><a href="#cb25-159"></a>The embedding of a token with id <span class="in">`i`</span> is simply the <span class="in">`i`</span>-th row of the embedding matrix. For example, the embedding of the token with id 0 is the first row of the embedding matrix. Mapping tokens to embeddings is plainly a lookup operation.</span>
<span id="cb25-160"><a href="#cb25-160"></a></span>
<span id="cb25-161"><a href="#cb25-161"></a><span class="fu">### V0</span></span>
<span id="cb25-162"><a href="#cb25-162"></a>Printing the embedding matrix of a made-up vocabulary. The embeddings are randomly initialized.</span>
<span id="cb25-163"><a href="#cb25-163"></a>The embeddings matrix size is determined by the vocabulary size and the embedding dimension. The embedding dimension is a hyperparameter that can be tuned.</span>
<span id="cb25-164"><a href="#cb25-164"></a></span>
<span id="cb25-165"><a href="#cb25-165"></a>Embeddings are tensors with a shape of (vocab_size, embedding_dim).</span>
<span id="cb25-168"><a href="#cb25-168"></a><span class="in">```{python}</span></span>
<span id="cb25-169"><a href="#cb25-169"></a><span class="co">#| echo: true</span></span>
<span id="cb25-170"><a href="#cb25-170"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb25-171"><a href="#cb25-171"></a>nn.Embedding(<span class="dv">10</span>, <span class="dv">4</span>).weight</span>
<span id="cb25-172"><a href="#cb25-172"></a><span class="in">```</span></span>
<span id="cb25-173"><a href="#cb25-173"></a></span>
<span id="cb25-174"><a href="#cb25-174"></a>Once we have the embeddings matrix, we can map input tokens to their corresponding embeddings.</span>
<span id="cb25-177"><a href="#cb25-177"></a><span class="in">```{python}</span></span>
<span id="cb25-178"><a href="#cb25-178"></a>stage1.embeddings.v0()</span>
<span id="cb25-179"><a href="#cb25-179"></a><span class="in">```</span></span>
<span id="cb25-180"><a href="#cb25-180"></a></span>
<span id="cb25-181"><a href="#cb25-181"></a><span class="fu">### V1</span></span>
<span id="cb25-182"><a href="#cb25-182"></a></span>
<span id="cb25-183"><a href="#cb25-183"></a><span class="ss">- </span>Data loaders create batches of token-ids for efficient processing</span>
<span id="cb25-184"><a href="#cb25-184"></a><span class="ss">- </span>Batching is valuable for large text corpora</span>
<span id="cb25-185"><a href="#cb25-185"></a><span class="ss">- </span>Batched processing leverages GPU acceleration when available</span>
<span id="cb25-186"><a href="#cb25-186"></a><span class="ss">- </span>Even on CPU, batching enables more efficient multi-threaded processing</span>
<span id="cb25-187"><a href="#cb25-187"></a><span class="ss">- </span>The example demonstrates creating embeddings for "The Verdict" text using previously created data loader with batch processing</span>
<span id="cb25-188"><a href="#cb25-188"></a></span>
<span id="cb25-191"><a href="#cb25-191"></a><span class="in">```{python}</span></span>
<span id="cb25-192"><a href="#cb25-192"></a>stage1.embeddings.v1()</span>
<span id="cb25-193"><a href="#cb25-193"></a><span class="in">```</span></span>
<span id="cb25-194"><a href="#cb25-194"></a></span>
<span id="cb25-195"><a href="#cb25-195"></a><span class="fu">### V2 - Using word positions</span></span>
<span id="cb25-196"><a href="#cb25-196"></a></span>
<span id="cb25-197"><a href="#cb25-197"></a><span class="ss">* </span>Taking the previous example and thinking further, while embeddings capture semantic relationships in a denser space, we also need to encode token positions in sentences.</span>
<span id="cb25-198"><a href="#cb25-198"></a><span class="ss">  * </span>A word's meaning can change based on its position</span>
<span id="cb25-199"><a href="#cb25-199"></a><span class="ss">  * </span>A sentence's meaning can change based on word order</span>
<span id="cb25-200"><a href="#cb25-200"></a><span class="ss">  * </span>We hypothesize this and hope our neural network architecture will learn it</span>
<span id="cb25-201"><a href="#cb25-201"></a></span>
<span id="cb25-202"><a href="#cb25-202"></a><span class="ss">* </span>We'll abstract away the details of position encoding implementation:</span>
<span id="cb25-203"><a href="#cb25-203"></a><span class="ss">  * </span>No clean notion of position exists (no defined start/end for text we process)</span>
<span id="cb25-204"><a href="#cb25-204"></a><span class="ss">  * </span>Focus on the current input and calculations relative to it</span>
<span id="cb25-205"><a href="#cb25-205"></a></span>
<span id="cb25-206"><a href="#cb25-206"></a><span class="ss">* </span>Even within the current input batch, position is a hazy concept due to sliding windows:</span>
<span id="cb25-207"><a href="#cb25-207"></a><span class="ss">  * </span>Options include using absolute positions within current sequence/batch</span>
<span id="cb25-208"><a href="#cb25-208"></a><span class="ss">  * </span>Or encoding relative positions instead</span>
<span id="cb25-209"><a href="#cb25-209"></a></span>
<span id="cb25-212"><a href="#cb25-212"></a><span class="in">```{python}</span></span>
<span id="cb25-213"><a href="#cb25-213"></a>stage1.embeddings.v2pos()</span>
<span id="cb25-214"><a href="#cb25-214"></a><span class="in">```</span></span>
<span id="cb25-215"><a href="#cb25-215"></a></span>
<span id="cb25-216"><a href="#cb25-216"></a></span>
<span id="cb25-217"><a href="#cb25-217"></a></span>
<span id="cb25-218"><a href="#cb25-218"></a></span>
<span id="cb25-219"><a href="#cb25-219"></a></span>
<span id="cb25-220"><a href="#cb25-220"></a></span>
<span id="cb25-221"><a href="#cb25-221"></a></span>
<span id="cb25-222"><a href="#cb25-222"></a></span>
<span id="cb25-223"><a href="#cb25-223"></a></span>
<span id="cb25-224"><a href="#cb25-224"></a></span>
<span id="cb25-225"><a href="#cb25-225"></a></span>
<span id="cb25-226"><a href="#cb25-226"></a></span>
<span id="cb25-227"><a href="#cb25-227"></a></span>
<span id="cb25-228"><a href="#cb25-228"></a></span>
<span id="cb25-229"><a href="#cb25-229"></a></span>
<span id="cb25-230"><a href="#cb25-230"></a></span>
<span id="cb25-231"><a href="#cb25-231"></a></span>
<span id="cb25-232"><a href="#cb25-232"></a></span>
<span id="cb25-233"><a href="#cb25-233"></a></span>
<span id="cb25-234"><a href="#cb25-234"></a></span>
<span id="cb25-235"><a href="#cb25-235"></a></span>
<span id="cb25-236"><a href="#cb25-236"></a></span>
<span id="cb25-237"><a href="#cb25-237"></a></span>
<span id="cb25-238"><a href="#cb25-238"></a></span>
<span id="cb25-239"><a href="#cb25-239"></a></span>
<span id="cb25-240"><a href="#cb25-240"></a></span>
<span id="cb25-241"><a href="#cb25-241"></a></span>
<span id="cb25-242"><a href="#cb25-242"></a></span>
<span id="cb25-243"><a href="#cb25-243"></a></span>
<span id="cb25-244"><a href="#cb25-244"></a></span>
<span id="cb25-245"><a href="#cb25-245"></a></span>
<span id="cb25-246"><a href="#cb25-246"></a></span>
<span id="cb25-247"><a href="#cb25-247"></a></span>
<span id="cb25-248"><a href="#cb25-248"></a></span>
<span id="cb25-249"><a href="#cb25-249"></a></span>
<span id="cb25-250"><a href="#cb25-250"></a></span>
</code><button title="Copy to Clipboard" class="code-copy-button" data-in-quarto-modal=""><i class="bi"></i></button></pre></div>
</div></div></div></div></div>
</div> <!-- /content -->




</body></html>