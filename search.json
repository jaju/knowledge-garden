[
  {
    "objectID": "blog/index.html",
    "href": "blog/index.html",
    "title": "Blog",
    "section": "",
    "text": "Welcome to the blog.\nBlog posts are published once and remain unchanged.\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "notes/neural-networks/index.html",
    "href": "notes/neural-networks/index.html",
    "title": "Neural Networks",
    "section": "",
    "text": "[!NOTE] This note evolves over time. Refactors expected.\n\n\nOverview"
  },
  {
    "objectID": "notes/index.html",
    "href": "notes/index.html",
    "title": "Notes",
    "section": "",
    "text": "Evolving notes. Content here is updated over time.\n\n\n\n\n\n\n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nNeural Networks\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNotes: LLM From Scratch\n\n\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "notes/llm-from-scratch/index.html",
    "href": "notes/llm-from-scratch/index.html",
    "title": "Notes: LLM From Scratch",
    "section": "",
    "text": "Notes and pointers during the exploration of the book LLM From Scratch by Sebastian Raschka. While I am using this book as the central reference, the notes will contain additional references and pointers to other resources.\nNote: Moving to quarto project"
  },
  {
    "objectID": "notes/llm-from-scratch/index.html#key-challenges-in-pre-training",
    "href": "notes/llm-from-scratch/index.html#key-challenges-in-pre-training",
    "title": "Notes: LLM From Scratch",
    "section": "2.1 Key challenges in pre-training",
    "text": "2.1 Key challenges in pre-training\n\nData: A good corpus of text is crucial for pre-training. A choice of corpus will depend on the downstream tasks.\nModel: The model architecture is crucial for pre-training. A good model architecture will help in learning the underlying structure of the data.\nBudget: Pre-training a large language model requires a lot of computational resources. A good budget is crucial for pre-training.\nGPT-3 potentially cost $4.6 million to train, per Li (2020).\n\nAfter pre-training, the model can be fine-tuned on a specific downstream task. Specifically, for following instructions, foundational models can be fine-tuned on a dataset of instructions. Ouyang et al. (2022) presented ideas on how to fine-tune GPT-3 on a dataset of instructions."
  },
  {
    "objectID": "notes/llm-from-scratch/index.html#tokenization",
    "href": "notes/llm-from-scratch/index.html#tokenization",
    "title": "Notes: LLM From Scratch",
    "section": "3.1 Tokenization",
    "text": "3.1 Tokenization\nThe more tokens we have, the more information we can capture. However, more tokens also mean more computational resources.\nExercise discussion: Why does the token vocabulary affect computation and how?\nEvery language has an alphabet. That’s the most fundamental unit of any language. The next level of abstraction is words. Words are made up of characters. We capture words in dictionaries, and rules to combines them in grammar. For a long time, traditional NLP has attempted to capture the meaning of words and sentences using dictionaries and grammar rules. However, this approach has limitations. For example, the same word can have different meanings in different contexts. The same meaning can be expressed using different words. The same word can be spelled differently. But human infants, as they grow, learn to understand language without dictionaries and grammar rules. This is a key observation that can help us understand how to approach language understanding using deep learning.\nWe create tokens not just for words, but also for subwords and characters. This allows us to capture more information. Secondly, we do not encode grammar rules (which are hard to define and may not be exhaustive or easy to update). Instead, we let the model learn the rules from the data. This is a key difference between traditional NLP and deep learning NLP.\nTo ensure we can capture patterns in the data, and generalize to unseen data, we approach tokenization as a data-driven process. We let the model learn the patterns in the data, rather than defining the patterns ourselves. And we allow for tokens to be defined and combined in ways that are not predefined by us. We also build in the ability to deal with unseen tokens and make sense of unseen data by invoking the context in which they appear.\n\n\nClick to expand/collapse\nimport importlib\nimport stage1.tokenization\nimportlib.reload(stage1.tokenization)\n\n\n&lt;module 'stage1.tokenization' from '/Users/jaju/github/knowledge-garden/notes/llm-from-scratch/stage1/tokenization.py'&gt;\n\n\n\n3.1.1 V1\nThe first, naive implementation that can not handle unseen tokens, as tokens are identified based on word boundaries from the training text.\n\n\nClick to expand/collapse\nstage1.tokenization.v1()\n\n\n\n2025-08-10 15:29:45.230 | INFO     | utils.downloaders:download:21 - data/the_verdict.txt already exists. Skipping download.\n\n\n\n\nError: Token 'draggees' not found in vocab.\n\n\n\n\n3.1.2 V2\nAbility to handle unseen tokens, plus additional signals to like begin/end of text, padding, etc. We simply preprocess the text to handle unseen tokens and replace then with a special token.\n\n\nClick to expand/collapse\nstage1.tokenization.v2()\n\n\n\n2025-08-10 15:29:45.235 | INFO     | utils.downloaders:download:21 - data/the_verdict.txt already exists. Skipping download.\n\n\n\n\n╒═════════╤════════════════════════════╕\n│ Input   │ three she draggees equally │\n├─────────┼────────────────────────────┤\n│ Encoded │ [1004, 876, 1131, 394]     │\n├─────────┼────────────────────────────┤\n│ Decoded │ three she &lt;|unk|&gt; equally  │\n╘═════════╧════════════════════════════╛\n\n\n\n\n3.1.3 V3 - Byte Pair Encoding (BPE)\nUsed in the original ChatGPT as well as GPT-2 and GPT-3. This one goes further granular in how it identifies tokens - encompassing all atomic units and then certain agglomerations of them. This is a data-driven approach to tokenization, where we learn the tokens from the data. We start with a vocabulary of all characters in the training text, and then iteratively merge the most frequent pairs of tokens until we reach a desired vocabulary size. This allows us to capture subword units and handle unseen tokens more effectively. tiktoken is a Python implementation of BPE. Implementation from scratch is not a key aim of this book, but it is useful to understand the concepts.\nAside: Listing encodings in the tiktoken library.\n\n\nClick to expand/collapse\nimport tiktoken\nprint(tiktoken.list_encoding_names())\n\n\n['gpt2', 'r50k_base', 'p50k_base', 'p50k_edit', 'cl100k_base', 'o200k_base']\n\n\nIn this example, we don’t create a new vocabulary, but use the ‘GPT-2’ vocabulary. (See source)\n\n\nClick to expand/collapse\nstage1.tokenization.v3()\n\n\n\n2025-08-10 15:29:45.243 | INFO     | utils.downloaders:download:21 - data/the_verdict.txt already exists. Skipping download.\n\n\n\n\n╒═════════╤════════════════════════════════════╕\n│ Input   │ three she draggees equally         │\n├─────────┼────────────────────────────────────┤\n│ Encoded │ [15542, 673, 6715, 469, 274, 8603] │\n├─────────┼────────────────────────────────────┤\n│ Decoded │ three she draggees equally         │\n╘═════════╧════════════════════════════════════╛\n\n\nNotice that the number of tokens is more than the number of distinct words in this example.\n\n\n3.1.4 V4\nThis is a further improvement over the previous version. “Improvement” not because of the tokenization technique but because of how we inject special symbols into the text. We use a special token to indicate the beginning and end of a sentence, and a special token to indicate padding.\n\n\nClick to expand/collapse\nstage1.tokenization.v4()\n\n\n\n2025-08-10 15:29:45.349 | INFO     | utils.downloaders:download:21 - data/the_verdict.txt already exists. Skipping download.\n\n\n\n\n╒═════════╤═══════════════════════════════════════════════════════════════════════════════╕\n│ Input   │ three she dragged equally. &lt;|endoftext|&gt; This is the end of the document.     │\n├─────────┼───────────────────────────────────────────────────────────────────────────────┤\n│ Encoded │ [15542, 673, 17901, 8603, 13, 220, 50256, 770, 318, 262, 886, 286, 262, 3188, │\n│         │ 13]                                                                           │\n├─────────┼───────────────────────────────────────────────────────────────────────────────┤\n│ Decoded │ three she dragged equally. &lt;|endoftext|&gt; This is the end of the document.     │\n╘═════════╧═══════════════════════════════════════════════════════════════════════════════╛\n\n\n\n3.1.4.1 BPE: A further demonstration\nThis is how BPE handles garbage text. And also, preserves spaces between tokens, because it makes no special assumptions about the text, including the notion of word boundaries.\n\n\nClick to expand/collapse\nstage1.tokenization.v4bpe()\n\n\n╒═════════╤═════════════════════════════════════════════════════════════════════════════════╕\n│ Input   │ asd asdjfkjsdf ksjfksa sdkfjsj   powiuosadoapofqfvv                             │\n├─────────┼─────────────────────────────────────────────────────────────────────────────────┤\n│ Encoded │ [292, 67, 355, 28241, 69, 74, 8457, 7568, 479, 82, 73, 69, 591, 64, 264, 34388, │\n│         │ 69, 8457, 73, 220, 220, 7182, 16115, 418, 4533, 499, 1659, 80, 69, 25093]       │\n├─────────┼─────────────────────────────────────────────────────────────────────────────────┤\n│ Decoded │ asd asdjfkjsdf ksjfksa sdkfjsj   powiuosadoapofqfvv                             │\n╘═════════╧═════════════════════════════════════════════════════════════════════════════════╛"
  },
  {
    "objectID": "notes/llm-from-scratch/index.html#embeddings",
    "href": "notes/llm-from-scratch/index.html#embeddings",
    "title": "Notes: LLM From Scratch",
    "section": "3.2 Embeddings",
    "text": "3.2 Embeddings\n\nTokens as 1-hot vectors create sparse matrices that are inefficient and fail to capture semantic relationships\nEmbeddings represent tokens in a lower-dimensional space that preserves semantic relationships\nEmbeddings are learned, not universal representations, and are specific to the model they’re trained in\nThe embedding space dimension is a tunable hyperparameter (larger = more informative but more resource-intensive)\nEmbeddings start as random values and are refined during training through gradient updates\n\n\n\nClick to expand/collapse\nimport importlib\nimport stage1.embeddings\nimportlib.reload(stage1.embeddings)\n\n\n&lt;module 'stage1.embeddings' from '/Users/jaju/github/knowledge-garden/notes/llm-from-scratch/stage1/embeddings.py'&gt;\n\n\n\n3.2.1 What do they look like?\nLet’s create a randomly initialized embedding matrix for a vocabulary of size 10 and an embedding dimension of 4. The embeddings matrix is then of size (10, 4), where each row corresponds to a token in the vocabulary and each column corresponds to a dimension in the embedding space.\n\n\nClick to expand/collapse\nimport torch\nimport stage1.embeddings\nembedding_dim = 4\nvocab_size = 10\n\nembeddings = torch.nn.Embedding(vocab_size, embedding_dim)\nprint(embeddings.weight)\nprint(f\"Embedding of token-id 2 is {embeddings(torch.tensor([1]))}\")\n\n\nParameter containing:\ntensor([[ 1.3359, -0.0304, -0.1601, -0.3360],\n        [-0.4213, -1.0907,  0.5489,  2.7181],\n        [ 0.0076, -0.3854,  0.8796,  0.7384],\n        [-0.7225, -1.3331,  1.7480,  0.2413],\n        [-0.2454,  0.0377,  0.9727,  0.4702],\n        [ 1.0761, -0.5090,  0.8463, -0.7448],\n        [-1.7504,  0.4115, -0.0096,  0.3085],\n        [ 0.3065, -1.3023, -1.0267, -1.5018],\n        [-0.8394, -1.9690, -1.8642,  0.0318],\n        [ 0.6605,  0.3686,  0.4146, -1.0726]], requires_grad=True)\nEmbedding of token-id 2 is tensor([[-0.4213, -1.0907,  0.5489,  2.7181]], grad_fn=&lt;EmbeddingBackward0&gt;)\n\n\nThe embedding of a token with id i is simply the i-th row of the embedding matrix. For example, the embedding of the token with id 0 is the first row of the embedding matrix. Mapping tokens to embeddings is plainly a lookup operation.\n\n\n3.2.2 V0\nPrinting the embedding matrix of a made-up vocabulary. The embeddings are randomly initialized. The embeddings matrix size is determined by the vocabulary size and the embedding dimension. The embedding dimension is a hyperparameter that can be tuned.\nEmbeddings are tensors with a shape of (vocab_size, embedding_dim).\n\n\nClick to expand/collapse\nimport torch.nn as nn\nnn.Embedding(10, 4).weight\n\n\nParameter containing:\ntensor([[ 0.6434, -1.3997,  0.5496, -0.2468],\n        [-1.0558, -0.7513,  1.1306,  0.5121],\n        [-0.2711, -0.6184, -1.3838,  0.7506],\n        [ 0.8909,  1.9996,  2.2013,  0.2673],\n        [-0.3967,  1.0333, -1.3575, -0.9402],\n        [ 0.4206,  1.3260, -0.6439,  0.5267],\n        [-0.8450, -0.7370,  1.5607, -0.1573],\n        [ 0.8441, -0.6949, -0.0823, -0.2533],\n        [ 0.4574,  0.6316, -0.4980, -0.4994],\n        [-0.0536,  2.0566,  1.0303, -0.7219]], requires_grad=True)\n\n\nOnce we have the embeddings matrix, we can map input tokens to their corresponding embeddings.\n\n\nClick to expand/collapse\nstage1.embeddings.v0()\n\n\nPseudo-randomly initialized embedding layer:\n╒══════════════════════════╤══════════════════════════════════════════════════════════════════════════════╕\n│ Vocab Size               │ 10                                                                           │\n├──────────────────────────┼──────────────────────────────────────────────────────────────────────────────┤\n│ Output Dimension         │ 3                                                                            │\n├──────────────────────────┼──────────────────────────────────────────────────────────────────────────────┤\n│ Embedding Layer          │ Embedding(10, 3)                                                             │\n├──────────────────────────┼──────────────────────────────────────────────────────────────────────────────┤\n│ Layer Weights            │ Parameter containing: tensor([[-1.1258, -1.1524, -0.2506],         [-0.4339, │\n│                          │ 0.8487,  0.6920],         [-0.3160, -2.1152,  0.3223],         [-1.2633,     │\n│                          │ 0.3500,  0.3081],         [ 0.1198,  1.2377, -0.1435],         [-0.1116,     │\n│                          │ -0.6136,  0.0316],         [-0.4927,  0.2484,  0.4397],         [ 0.1124,    │\n│                          │ -0.8411, -2.3160],         [-0.1023,  0.7924, -0.2897],         [ 0.0525,    │\n│                          │ 0.5229,  2.3022]], requires_grad=True)                                       │\n├──────────────────────────┼──────────────────────────────────────────────────────────────────────────────┤\n│ Embedding for token_id 5 │ tensor([-0.1116, -0.6136,  0.0316], grad_fn=&lt;EmbeddingBackward0&gt;)            │\n╘══════════════════════════╧══════════════════════════════════════════════════════════════════════════════╛\n╒═══════════════════════════════════╤═══════════════════════════════════════════════════════════════════════════╕\n│ Embedding for token_ids [2, 4, 6] │ tensor([[-0.3160, -2.1152,  0.3223],         [ 0.1198,  1.2377, -0.1435], │\n│                                   │ [-0.4927,  0.2484,  0.4397]], grad_fn=&lt;EmbeddingBackward0&gt;)               │\n╘═══════════════════════════════════╧═══════════════════════════════════════════════════════════════════════════╛\n\n\n\n\n3.2.3 V1\n\nData loaders create batches of token-ids for efficient processing\nBatching is valuable for large text corpora\nBatched processing leverages GPU acceleration when available\nEven on CPU, batching enables more efficient multi-threaded processing\nThe example demonstrates creating embeddings for “The Verdict” text using previously created data loader with batch processing\n\n\n\nClick to expand/collapse\nstage1.embeddings.v1()\n\n\n╒════════════════╤═══════╕\n│ vocab_size     │ 50257 │\n├────────────────┼───────┤\n│ embedding_size │   256 │\n├────────────────┼───────┤\n│ input_length   │     4 │\n├────────────────┼───────┤\n│ batch_size     │     8 │\n╘════════════════╧═══════╛\nEmbedding Layer: Parameter containing:\ntensor([[ 0.9383,  0.4889, -0.6731,  ...,  1.2948,  1.4628, -0.6204],\n        [ 0.6257, -1.2231, -0.6232,  ...,  0.3260,  0.5352,  1.9733],\n        [-1.4115, -1.0295,  0.1267,  ...,  0.5027, -0.8871,  1.9974],\n        ...,\n        [ 0.6928, -0.5382, -0.8726,  ..., -0.5148,  0.9695,  0.7689],\n        [-0.5866,  0.6971,  1.8386,  ...,  0.4298, -0.5139,  1.6624],\n        [ 0.6073,  0.2991,  0.7669,  ..., -1.3811, -1.4284, -0.5630]],\n       requires_grad=True)\nInputs shape: torch.Size([8, 4])\nTargets shape: torch.Size([8, 4])\nInput tensor shape:  torch.Size([1, 4])\nInput Tensor: tensor([[ 6020, 35217, 39936, 24254]])\nOutput tensor shape:  torch.Size([1, 4, 256])\nOutput Tensor: tensor([[[-0.3459, -1.3498, -2.2147,  ..., -0.7331, -1.2353,  0.4724],\n         [-1.2983, -1.1385, -0.2726,  ...,  1.0465, -0.3054, -0.0303],\n         [-0.9905,  0.4149, -0.1217,  ...,  2.3362, -0.5502,  0.3072],\n         [ 2.0188,  0.2669, -0.0151,  ..., -0.1302,  0.0308, -0.0452]]],\n       grad_fn=&lt;EmbeddingBackward0&gt;)\n\n\n\n\n3.2.4 V2 - Using word positions\n\nTaking the previous example and thinking further, while embeddings capture semantic relationships in a denser space, we also need to encode token positions in sentences.\n\nA word’s meaning can change based on its position\nA sentence’s meaning can change based on word order\nWe hypothesize this and hope our neural network architecture will learn it\n\nWe’ll abstract away the details of position encoding implementation:\n\nNo clean notion of position exists (no defined start/end for text we process)\nFocus on the current input and calculations relative to it\n\nEven within the current input batch, position is a hazy concept due to sliding windows:\n\nOptions include using absolute positions within current sequence/batch\nOr encoding relative positions instead\n\n\n\n\nClick to expand/collapse\nstage1.embeddings.v2pos()\n\n\n╒════════════════╤═══════╕\n│ vocab_size     │ 50257 │\n├────────────────┼───────┤\n│ embedding_size │   256 │\n├────────────────┼───────┤\n│ input_length   │     4 │\n├────────────────┼───────┤\n│ batch_size     │     8 │\n╘════════════════╧═══════╛\nEmbedding Layer: Parameter containing:\ntensor([[ 0.7116, -0.3658,  0.7480,  ..., -1.1679, -0.2427,  1.4958],\n        [-0.4648, -0.9674, -0.0083,  ..., -2.7200,  0.1168, -0.6456],\n        [-0.2261,  0.3726,  0.3099,  ...,  0.1685, -1.2439,  0.0087],\n        ...,\n        [ 0.8634, -0.9374,  0.6470,  ..., -0.7115, -1.4025,  1.2715],\n        [ 1.0015,  1.5188,  0.0704,  ...,  0.0646,  0.2980, -1.3077],\n        [ 0.1837,  0.0658,  0.3290,  ..., -0.3405, -0.9584, -0.3902]],\n       requires_grad=True)\nPositional Embedding Layer: Parameter containing:\ntensor([[-0.0436,  0.8686, -1.2746,  ...,  0.2960, -0.6259, -0.0991],\n        [-1.3748,  1.1202,  1.5418,  ...,  0.0582,  0.6032, -0.9257],\n        [ 0.5589, -0.6705, -0.4068,  ..., -0.9972,  0.0822, -0.0294],\n        [-1.1645, -0.5344,  1.6700,  ...,  1.5051, -1.4901,  0.0909]],\n       requires_grad=True)\nInput tokens:  tensor([[   40,   367,  2885,  1464],\n        [  367,  2885,  1464,  1807],\n        [ 2885,  1464,  1807,  3619],\n        [ 1464,  1807,  3619,   402],\n        [ 1807,  3619,   402,   271],\n        [ 3619,   402,   271, 10899],\n        [  402,   271, 10899,  2138],\n        [  271, 10899,  2138,   257]])\nTarget tokens:  tensor([[  367,  2885,  1464,  1807],\n        [ 2885,  1464,  1807,  3619],\n        [ 1464,  1807,  3619,   402],\n        [ 1807,  3619,   402,   271],\n        [ 3619,   402,   271, 10899],\n        [  402,   271, 10899,  2138],\n        [  271, 10899,  2138,   257],\n        [10899,  2138,   257,  7026]])\nInput shape:  torch.Size([8, 4, 256])\nPlain token embeddings:  tensor([[[-1.0176,  0.5446, -0.3825,  ...,  0.7349, -0.3825, -0.1575],\n         [-0.1834, -1.1158,  2.0952,  ...,  0.9170,  0.9610,  0.5506],\n         [ 0.3635,  1.8486,  0.0130,  ...,  1.5528,  0.7154, -0.4855],\n         [ 1.3264, -0.4894, -0.0092,  ..., -2.1829, -0.2430,  1.1335]],\n\n        [[-0.1834, -1.1158,  2.0952,  ...,  0.9170,  0.9610,  0.5506],\n         [ 0.3635,  1.8486,  0.0130,  ...,  1.5528,  0.7154, -0.4855],\n         [ 1.3264, -0.4894, -0.0092,  ..., -2.1829, -0.2430,  1.1335],\n         [ 0.5092,  1.0814, -0.2302,  ...,  0.2658,  0.1148, -0.0699]],\n\n        [[ 0.3635,  1.8486,  0.0130,  ...,  1.5528,  0.7154, -0.4855],\n         [ 1.3264, -0.4894, -0.0092,  ..., -2.1829, -0.2430,  1.1335],\n         [ 0.5092,  1.0814, -0.2302,  ...,  0.2658,  0.1148, -0.0699],\n         [-0.4399,  0.5992, -0.8237,  ...,  0.7319,  0.8932,  0.7449]],\n\n        ...,\n\n        [[-0.4399,  0.5992, -0.8237,  ...,  0.7319,  0.8932,  0.7449],\n         [-0.6520,  0.1563,  0.0860,  ...,  1.9872, -0.1324, -0.4514],\n         [-1.4690, -0.5862, -0.1399,  ..., -2.0051,  0.8762,  0.3288],\n         [-1.5371,  1.3817,  2.3762,  ..., -2.0132, -0.2221,  1.1241]],\n\n        [[-0.6520,  0.1563,  0.0860,  ...,  1.9872, -0.1324, -0.4514],\n         [-1.4690, -0.5862, -0.1399,  ..., -2.0051,  0.8762,  0.3288],\n         [-1.5371,  1.3817,  2.3762,  ..., -2.0132, -0.2221,  1.1241],\n         [-1.4999, -0.5814, -0.8857,  ..., -0.5622,  1.1242, -0.6741]],\n\n        [[-1.4690, -0.5862, -0.1399,  ..., -2.0051,  0.8762,  0.3288],\n         [-1.5371,  1.3817,  2.3762,  ..., -2.0132, -0.2221,  1.1241],\n         [-1.4999, -0.5814, -0.8857,  ..., -0.5622,  1.1242, -0.6741],\n         [ 1.6521,  0.8120,  0.2671,  ...,  0.5159,  0.5164, -0.2825]]],\n       grad_fn=&lt;EmbeddingBackward0&gt;)\nPositional embeddings:  tensor([[[-1.0611,  1.4131, -1.6572,  ...,  1.0309, -1.0084, -0.2566],\n         [-1.5583,  0.0044,  3.6370,  ...,  0.9752,  1.5642, -0.3751],\n         [ 0.9224,  1.1781, -0.3938,  ...,  0.5557,  0.7976, -0.5149],\n         [ 0.1619, -1.0238,  1.6608,  ..., -0.6777, -1.7331,  1.2244]],\n\n        [[-0.2270, -0.2473,  0.8206,  ...,  1.2130,  0.3351,  0.4515],\n         [-1.0113,  2.9688,  1.5548,  ...,  1.6110,  1.3186, -1.4112],\n         [ 1.8853, -1.1599, -0.4160,  ..., -3.1800, -0.1608,  1.1042],\n         [-0.6554,  0.5470,  1.4398,  ...,  1.7710, -1.3753,  0.0210]],\n\n        [[ 0.3199,  2.7172, -1.2616,  ...,  1.8489,  0.0895, -0.5846],\n         [-0.0484,  0.6308,  1.5326,  ..., -2.1247,  0.3602,  0.2078],\n         [ 1.0681,  0.4109, -0.6370,  ..., -0.7314,  0.1969, -0.0992],\n         [-1.6045,  0.0648,  0.8463,  ...,  2.2370, -0.5969,  0.8358]],\n\n        ...,\n\n        [[-0.4835,  1.4678, -2.0983,  ...,  1.0279,  0.2673,  0.6458],\n         [-2.0268,  1.2765,  1.6278,  ...,  2.0454,  0.4708, -1.3771],\n         [-0.9101, -1.2567, -0.5467,  ..., -3.0022,  0.9584,  0.2994],\n         [-2.7016,  0.8474,  4.0462,  ..., -0.5081, -1.7122,  1.2150]],\n\n        [[-0.6956,  1.0248, -1.1886,  ...,  2.2833, -0.7583, -0.5505],\n         [-2.8438,  0.5340,  1.4020,  ..., -1.9469,  1.4794, -0.5969],\n         [-0.9782,  0.7112,  1.9694,  ..., -3.0104, -0.1400,  1.0948],\n         [-2.6645, -1.1158,  0.7843,  ...,  0.9429, -0.3659, -0.5832]],\n\n        [[-1.5126,  0.2823, -1.4145,  ..., -1.7091,  0.2503,  0.2297],\n         [-2.9119,  2.5019,  3.9180,  ..., -1.9550,  0.3811,  0.1984],\n         [-0.9410, -1.2519, -1.2925,  ..., -1.5594,  1.2064, -0.7035],\n         [ 0.4875,  0.2776,  1.9372,  ...,  2.0210, -0.9737, -0.1916]]],\n       grad_fn=&lt;AddBackward0&gt;)"
  },
  {
    "objectID": "blog/2025/2025-08-10-hello-world.html",
    "href": "blog/2025/2025-08-10-hello-world.html",
    "title": "M'Sync",
    "section": "",
    "text": "title: “Hello World” date: !!str 2025-08-10 aliases: [/blog/hello-world.html] — Hello World This is a new blog"
  }
]